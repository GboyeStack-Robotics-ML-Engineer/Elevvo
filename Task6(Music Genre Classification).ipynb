{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1032238,"sourceType":"datasetVersion","datasetId":568973}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q livelossplot","metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing the Required Libraries","metadata":{}},{"cell_type":"code","source":"import random, os, glob \nimport numpy as np \nimport tensorflow as tf\nfrom keras.models import Sequential \nfrom keras.layers import Dropout, Dense, Conv2D, MaxPool2D, Flatten, Reshape, BatchNormalization, GlobalAveragePooling2D # layers I will incorporate\nfrom keras.callbacks import EarlyStopping \nfrom tensorflow.keras.applications import VGG19 \nfrom keras import backend\nfrom livelossplot import PlotLossesKeras \nimport librosa \nfrom librosa.display import specshow\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Defining Utility Functions","metadata":{}},{"cell_type":"code","source":"def setRandom():\n    seed = 0 # random seed value\n    os.environ[\"PYTHONHASHSEED\"] = str(seed) # if this is not set, a random value is used to seed the hashes of some objects\n    random.seed(seed) # sets the base python and numpy random seeds\n    np.random.seed(seed)\n    tf.random.set_seed(seed) # sets the tensorflow random seed\n    tf.compat.v1.set_random_seed(seed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# __Preparing Data__\n\n> To fulfil the aim of this investigation, I need to predict a song's genre from a randomly chosen 30-second snippet. This means that the dependent variable (the one we are attempting to measure) is the __*song genre*__.The first step in Preparing the dataset is Removing samples that will most likely cause errors when training\n\n\n## 1) Removing Erroneous Values\n> To ensure that the dataframe is consistent and will not cause errors in the future, I need to check for null or missing values.","metadata":{}},{"cell_type":"code","source":"source = \"../input/gtzan-dataset-music-genre-classification/Data/images_original/\" # source folder path\ngenres = [\"blues\", \"classical\", \"country\", \"disco\", \"hiphop\", \"jazz\", \"metal\", \"pop\", \"reggae\", \"rock\"] # list with the genre folder names\n\nfor genre in genres: # iterate through each genre folder\n    path = os.path.join(source, genre)\n    pngs = [i for i in os.listdir(path) if i[-4:] == \".png\"] # get a list of .png files in the genre folder\n    print(f\"Size of {genre} dataset: {len(pngs)} files.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Splitting the Data\n\n> To ensure reliable model evaluation, the dataset is divided into three parts â€” training, validation, and testing. This separation allows the model to learn from one subset, tune its parameters on another, and finally be evaluated on completely unseen data, providing a true measure of its generalization capability in real-world situations.For this work, an 80:9:10 ratio is used for the training, validation, and testing sets respectively, as illustrated in the dictionaries shown below.\n","metadata":{}},{"cell_type":"code","source":"setRandom()\nsplit = [80, 9, 10]\ntrain, val, test = {}, {}, {} # empty dictionaries to store the filepaths\ntrainLen, valLen, testLen = {}, {}, {} # empty dictionaries to store the number of files under each genre for each dataset\ndictionaries = [train, val, test]\n\nfor d in dictionaries:\n    if d == train: num = slice(0, split[0])\n    elif d == val: num = slice(split[0], split[0] + split[1])\n    else: num = slice(split[0] + split[1], split[0] + split[1] + split[2])\n    for genre in genres: # iterate through each genre folder\n        path = os.path.join(source, genre)\n        pngs = glob.glob(os.path.join(path, \"*.png\")) # get a list of .png filepaths in the genre folder\n        selected = pngs[num] # take the first 80 files\n        d[genre] = selected # store the selected files in the dictionary\n\nlenDictionaries = [{genre: len(d[genre]) for genre in genres} for d in dictionaries]        \n\nprint(f\"\\033[1mTraining:\\033[0m {lenDictionaries[0]}\")\nprint(f\"\\033[1mValidation:\\033[0m {lenDictionaries[1]}\")\nprint(f\"\\033[1mTest:\\033[0m {lenDictionaries[2]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, glob\nfrom random import shuffle\n\n# Shuffle the data for randomness\nsetRandom()\nsplit_ratio = {'train': 80, 'val': 9, 'test': 10}\n\n# Initialize dictionaries to store filepaths and counts\ndatasets = {'train': {}, 'val': {}, 'test': {}}\n\n# Loop through each genre folder\nfor genre in genres:\n    genre_path = os.path.join(source, genre)\n    image_paths = glob.glob(os.path.join(genre_path, \"*.png\"))\n    total_images = len(image_paths)\n\n    shuffle(image_paths)\n\n    # Calculate split indices\n    train_end = int((split_ratio['train'] / 100) * total_images)\n    val_end = train_end + int((split_ratio['val'] / 100) * total_images)\n\n    # Split images into train, val, test\n    datasets['train'][genre] = image_paths[:train_end]\n    datasets['val'][genre] = image_paths[train_end:val_end]\n    datasets['test'][genre] = image_paths[val_end:]\n\n# Count files per genre for each dataset\ndataset_lengths = {\n                    split: {genre: len(datasets[split][genre]) \n                            for genre in genres}\n                                for split in datasets\n                    }\n\n\nprint(f\"\\033[1mTraining:\\033[0m {dataset_lengths['train']}\")\nprint(f\"\\033[1mValidation:\\033[0m {dataset_lengths['val']}\")\nprint(f\"\\033[1mTest:\\033[0m {dataset_lengths['test']}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Formatting\n> The code below prepares the dataset so a TensorFlow model can read and train on it, and the prep() function applies the optimizations recommended in the Kaggle Computer Vision course..","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport os\n\nbatchSize = 32  # typical batch size for a neural network\n\ngenreMap = {\n    \"blues\": 0, \"classical\": 1, \"country\": 2, \"disco\": 3, \"hiphop\": 4,\n    \"jazz\": 5, \"metal\": 6, \"pop\": 7, \"reggae\": 8, \"rock\": 9\n}\ninverseGenreMap = {v: k for k, v in genreMap.items()}\n\nIMAGE_SIZE = (288, 432)  # (height, width)\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ndef _load_and_preprocess(path, label):\n    \"\"\"Read an image from disk, decode, resize and normalize to [0,1].\"\"\"\n    image = tf.io.read_file(path)\n    image = tf.image.decode_png(image, channels=3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, tf.float32) / 255.0\n    return image, label\n\ndef create_dataset_from_dict(d: dict, shuffle_buffer: int = 1000):\n    \"\"\"\n    Create a tf.data.Dataset from a dictionary mapping genres -> list_of_paths.\n    This keeps images on-disk and loads them lazily (more memory friendly).\n    \"\"\"\n    paths, labels = [], []\n    for genre, filepaths in d.items():\n        paths.extend(filepaths)\n        labels.extend([genreMap[genre]] * len(filepaths))\n\n    # Build dataset from paths and integer labels\n    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n    ds = ds.shuffle(buffer_size=min(shuffle_buffer, max(1, len(paths))))\n\n    # Map to actual images and preprocess\n    ds = ds.map(lambda p, l: _load_and_preprocess(p, l), num_parallel_calls=AUTOTUNE)\n    ds = ds.batch(batchSize, drop_remainder=False)\n    return ds\n\ndef prep(ds: tf.data.Dataset):\n    \"\"\"\n    Apply final dataset optimizations recommended for TF pipelines:\n    - ensures dtype is float32\n    - caches examples (in memory or disk)\n    - prefetches for performance\n    \"\"\"\n    ds = ds.map(lambda images, labels: (tf.image.convert_image_dtype(images, tf.float32), labels),\n                num_parallel_calls=AUTOTUNE)\n    ds = ds.cache()\n    ds = ds.prefetch(AUTOTUNE)\n    return ds\n\n# Create and prepare datasets (assuming train, val, test dicts exist)\ntraining = prep(create_dataset_from_dict(train))\nvalidation = prep(create_dataset_from_dict(val))\ntesting = prep(create_dataset_from_dict(test))\n\nprint(\"Datasets created.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below are some examples of the spectrograms in each genre for each dataset. There are some clearly noticeable patterns in the data suchas the fairly consistently loud nature of metal, but as usual there are still a few outliers most notably in blues and pop (both of which are quite wide genres).","metadata":{}},{"cell_type":"code","source":"def view_dataset(dataset):\n    genreExamples = {}  # dictionary to store examples for each label\n    \n    for images, labels in dataset:\n        for image, label in zip(images, labels):\n            label = int(label.numpy())  # convert label tensor to integer\n            if label not in genreExamples:\n                genreExamples[label] = image\n                \n        if len(genreExamples) == len(genres):\n            break\n    \n    # display the randomly chosen examples\n    plt.figure(figsize = (30, 20))\n    for label, image in genreExamples.items():\n        ax = plt.subplot(1, len(genres), label + 1)\n        plt.imshow(image)\n        plt.title(inverseGenreMap[label])\n        plt.axis(\"off\")\n    plt.show()\n\nprint(\"\\033[1mTraining Examples:\\033[0m\"); view_dataset(training) # shows a labelled example of a mel spectrogram from each genre from each dataset\nprint(\"\\033[1mValidation Examples:\\033[0m\"); view_dataset(validation)\nprint(\"\\033[1mTesting Examples:\\033[0m\"); view_dataset(testing)","metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing and Data Augmentation\n\n> Because of the nature of the dataset, applying data augmentation would be inappropriate. All spectrogram images share the same structure and formatting, so transformations would be unnecessary and could even negatively affect training. Therefore, data augmentation is intentionally omitted in this task.As part of preprocessing, the pixel values of the images were normalized to a range between 0 and 1 by dividing each RGB value by 255.0. This normalization step is crucial for enabling the models to properly interpret and learn from the dataset.\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Choosing-and-Training-a-Model\"></a>\n# __Choosing and Training a Model__\n\n\n#### **Transfer Learning Model with InceptionV3 Base (*CNN*)**\n\n> * After researching and testing with a number of pre-trained bases, the InceptionV3 architecture appeared the most promising for this application. * I will be using the default ImageNet weights as they have already learnt to effectively extract hierarchical features and other abstractions from images. Furthermore, ImageNet weights are typically optimised for detecting spatial patterns in images, which would be helpful for detecting patterns in frequency and volume over time.* The weights will also be frozen to retain valuable pre-learnt features, prevent overfitting, reduce computational cost, and ensure consistency. Doing this leverages the power of pre-trained models while allowing you to fine-tune only the top layers for your specific task. \n\n\n#### **Custom Convolutional Neural Network (*CNN*)**\n\n> * By designing my own base, I'm hoping that I will be able to compete with the transfer model.The following code is the result of hours of hyperparameter tuning and layer customisation in both models.\n","metadata":{}},{"cell_type":"markdown","source":"# **Transfer Learning Model with InceptionV3 Base (*CNN*)**","metadata":{}},{"cell_type":"code","source":"inputShape = [288, 432, 3] # the shape of the images (288px tall, 432px wide, and 3 colour channels/RGB)\n\nearlyStopping = EarlyStopping( \n    min_delta = 0.001,\n    patience = 20, \n    restore_best_weights = True \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import InceptionV3 # transfer learning model\nbaseModel = InceptionV3(input_shape = inputShape, weights = \"imagenet\", include_top = False, pooling = \"avg\")\n\nfor layer in baseModel.layers:\n    layer.trainable = False # freeze the pre-trained layers\n\ntransfer = Sequential([\n    baseModel,\n    \n    Flatten(),\n    BatchNormalization(),\n    Dense(512, activation = \"relu\"),\n    Dropout(0.3),\n    Dense(256, activation = \"relu\"),\n    Dropout(0.3), # dropout layer to prevent overfitting\n    Dense(128, activation = \"relu\"),\n    Dropout(0.3),\n    Dense(len(genres), activation = \"softmax\")\n])\n\noptimiser = tf.keras.optimizers.SGD(learning_rate = 0.0001)\n\ntransfer.compile(optimiser, loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])\ntransfer.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"setRandom()\nfrom livelossplot import PlotLossesKeras\ntransferHistory = transfer.fit(training, validation_data = validation, batch_size = batchSize, epochs = 2, verbose = 1, callbacks = [earlyStopping, PlotLossesKeras()])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Custom Convolutional Neural Network (*CNN*)**\n","metadata":{}},{"cell_type":"code","source":"cnn = Sequential([\n    BatchNormalization(input_shape = inputShape),\n    \n    Conv2D(32, (3, 3), activation = \"relu\"),\n    MaxPool2D((2, 2)),\n    \n    Conv2D(64, (3, 3), activation = \"relu\"),\n    MaxPool2D((2, 2)),\n    \n    Conv2D(128, (3, 3), activation = \"relu\"),\n    MaxPool2D((2, 2)),\n    \n    Conv2D(256, (3, 3), activation = \"relu\"),\n    MaxPool2D((2, 2)),\n    \n    Conv2D(512, (3, 3), activation = \"relu\"),\n    MaxPool2D((2, 2)),\n    \n    Flatten(),\n    Dense(1024, activation = \"relu\"),\n    Dropout(0.5),\n    Dense(512, activation = \"relu\"),\n    Dropout(0.5),\n    BatchNormalization(),\n    Dense(len(genres), activation = \"softmax\")\n])\n\noptimiser = tf.keras.optimizers.SGD(learning_rate = 0.001)\noptimiser.learning_rate.assign(0.01)\n\ncnn.compile(optimiser, loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]) # \"sparse_categorical_crossentropy\" because labels are integers\ncnn.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"setRandom()\ncnn.fit(training, validation_data = validation, batch_size = batchSize, epochs = 2, verbose = 1, callbacks = [earlyStopping, PlotLossesKeras()])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"Test-Data-Predictions\"></a>\n# __Test Data Predictions__\nBelow is the code to visually represent the accuracy of both models through confusion matrices.","metadata":{}},{"cell_type":"code","source":"def confusionMatrix(model, name):\n    trueLabels = np.concatenate([y for x, y in testing], axis = 0) # get the true labels from the testing dataset\n\n    predictedLabels = np.argmax(model.predict(testing, verbose = 0), axis = 1) # get the predicted labels from the model\n\n    matrix = confusion_matrix(trueLabels, predictedLabels) # create the confusion matrix\n\n    plt.figure() # plot the confusion matrix using seaborn for the heatmap\n    sns.heatmap(matrix, annot = True, cmap = \"Greens\", xticklabels = genres, yticklabels = genres)\n    plt.xlabel(\"Predicted Genre\")\n    plt.ylabel(\"True Genre\")\n    plt.title(f\"{name} Model: Confusion Matrix\")\n    plt.show()\n    \n    trainStats, valStats, testStats = model.evaluate(training, verbose = 0), model.evaluate(validation, verbose = 0), model.evaluate(testing, verbose = 0)\n    print(f\"\\033[1m{name} Model\\033[0m\")\n    print(f\"Training Accuracy: {round(trainStats[1] * 100, 4)}% \\nTrain Loss: {round(trainStats[0], 4)}\\n\")\n    print(f\"Validation Accuracy: {round(valStats[1] * 100, 4)}% \\nTest Loss: {round(valStats[0], 4)}\\n\")\n    print(f\"Testing Accuracy: {round(testStats[1] * 100, 4)}% \\nTest Loss: {round(testStats[0], 4)}\")\n\nconfusionMatrix(transfer, \"Transfer\")\n# confusionMatrix(cnn, \"Custom CNN\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}